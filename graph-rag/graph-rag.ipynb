{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph - RAG chatbot\n",
    "\n",
    "# Insert information into vector and graph database\n",
    "\n",
    "### Initializing databases and parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import psycopg2\n",
    "from neo4j import GraphDatabase\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "INPUT_DIR = os.environ.get(\"INPUT_DIR\", \"data\")\n",
    "EMBED_MODEL = os.environ.get(\"EMBED_MODEL\", \"text-embedding-3-small\")\n",
    "EMBED_DIM = int(os.environ.get(\"EMBED_DIM\", \"1536\"))\n",
    "LLAMA_API_KEY = os.environ[\"LLAMA_CLOUD_API_KEY\"]\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "PG = {\n",
    "    \"host\": os.environ.get(\"POSTGRES_HOST\", \"localhost\"),\n",
    "    \"port\": int(os.environ.get(\"POSTGRES_PORT\", \"5432\")),\n",
    "    \"dbname\": os.environ.get(\"POSTGRES_DB\", \"lawdb\"),\n",
    "    \"user\": os.environ.get(\"POSTGRES_USER\", \"postgres\"),\n",
    "    \"password\": os.environ.get(\"POSTGRES_PASSWORD\", \"postgres\"),\n",
    "}\n",
    "NEO4J_URI = os.environ.get(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_USER = os.environ.get(\"NEO4J_USERNAME\", \"neo4j\")\n",
    "NEO4J_PASS = os.environ.get(\"NEO4J_PASSWORD\", \"neo4j\")\n",
    "SUPPORTED_EXTS = {\".pdf\", \".docx\", \".pptx\", \".xlsx\", \".csv\"}\n",
    "\n",
    "def mk_uuid() -> uuid.UUID:\n",
    "    return uuid.uuid4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(client: OpenAI, texts: List[str]) -> List[List[float]]:\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    return [d.embedding for d in resp.data]\n",
    "\n",
    "def parse_with_llamaparse(input_dir: str) -> List[Dict[str, Any]]:\n",
    "    parser = LlamaParse(api_key=LLAMA_API_KEY, result_type=\"json\", language=\"no\", num_workers=4)\n",
    "    file_extractor = {ext: parser for ext in SUPPORTED_EXTS}\n",
    "    docs = SimpleDirectoryReader(input_dir=input_dir, file_extractor=file_extractor).load_data()\n",
    "    per_page_chunks: List[Dict[str, Any]] = []\n",
    "    for d in docs:\n",
    "        meta = d.metadata or {}\n",
    "        file_name = meta.get(\"file_name\") or meta.get(\"filename\") or \"unknown\"\n",
    "        title = meta.get(\"title\")\n",
    "        page_count = meta.get(\"page_count\")\n",
    "        json_obj = None\n",
    "        if d.text:\n",
    "            try:\n",
    "                json_obj = json.loads(d.text)\n",
    "            except Exception:\n",
    "                json_obj = None\n",
    "        if json_obj and isinstance(json_obj, dict) and \"pages\" in json_obj:\n",
    "            pages = json_obj.get(\"pages\") or []\n",
    "            for p in pages:\n",
    "                page_number = p.get(\"page_number\")\n",
    "                text = p.get(\"text\") or \"\"\n",
    "                if not text.strip():\n",
    "                    continue\n",
    "                per_page_chunks.append({\n",
    "                    \"file_name\": file_name,\n",
    "                    \"title\": title,\n",
    "                    \"page_count\": page_count,\n",
    "                    \"page_number\": page_number,\n",
    "                    \"chunk_index\": 0,\n",
    "                    \"text\": text,\n",
    "                })\n",
    "        else:\n",
    "            text = d.text or \"\"\n",
    "            if text.strip():\n",
    "                per_page_chunks.append({\n",
    "                    \"file_name\": file_name,\n",
    "                    \"title\": title,\n",
    "                    \"page_count\": page_count,\n",
    "                    \"page_number\": None,\n",
    "                    \"chunk_index\": 0,\n",
    "                    \"text\": text,\n",
    "                })\n",
    "    return per_page_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for inserting into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_document(cur, doc_row):\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO documents (id, file_name, title, page_count)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "        ON CONFLICT (id) DO UPDATE SET\n",
    "          file_name = EXCLUDED.file_name,\n",
    "          title = EXCLUDED.title,\n",
    "          page_count = EXCLUDED.page_count\n",
    "        \"\"\",\n",
    "        (str(doc_row[\"id\"]), doc_row[\"file_name\"], doc_row[\"title\"], doc_row[\"page_count\"]),\n",
    "    )\n",
    "\n",
    "def insert_chunk(cur, chunk_row):\n",
    "    vec = \"[\" + \",\".join(f\"{x:.8f}\" for x in chunk_row[\"embedding\"]) + \"]\"\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO doc_chunks (id, doc_id, page_number, chunk_index, chunk_text, embedding)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (id) DO NOTHING\n",
    "        \"\"\",\n",
    "        (\n",
    "            str(chunk_row[\"id\"]),\n",
    "            str(chunk_row[\"doc_id\"]),\n",
    "            chunk_row[\"page_number\"],\n",
    "            chunk_row[\"chunk_index\"],\n",
    "            chunk_row[\"text\"],\n",
    "            vec,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def neo4j_upsert_document(session, doc_row):\n",
    "    session.run(\n",
    "        \"\"\"\n",
    "        MERGE (d:Document {id: $id})\n",
    "        SET d.file_name = $file_name,\n",
    "            d.title = $title,\n",
    "            d.page_count = $page_count\n",
    "        \"\"\",\n",
    "        id=str(doc_row[\"id\"]),\n",
    "        file_name=doc_row[\"file_name\"],\n",
    "        title=doc_row[\"title\"],\n",
    "        page_count=doc_row[\"page_count\"],\n",
    "    )\n",
    "\n",
    "def neo4j_upsert_page_and_chunk(session, doc_row, chunk_row):\n",
    "    session.run(\n",
    "        \"\"\"\n",
    "        MATCH (d:Document {id: $doc_id})\n",
    "        MERGE (p:Page {doc_id: $doc_id, number: $page_number})\n",
    "          ON CREATE SET p.created_at = timestamp()\n",
    "        MERGE (d)-[:HAS_PAGE]->(p)\n",
    "        MERGE (c:Chunk {doc_id: $doc_id, page_number: $page_number, chunk_index: $chunk_index})\n",
    "          ON CREATE SET c.created_at = timestamp()\n",
    "        SET c.text = $text,\n",
    "            c.length = $length\n",
    "        MERGE (p)-[:HAS_CHUNK {index: $chunk_index}]->(c)\n",
    "        \"\"\",\n",
    "        doc_id=str(doc_row[\"id\"]),\n",
    "        page_number=chunk_row[\"page_number\"],\n",
    "        chunk_index=chunk_row[\"chunk_index\"],\n",
    "        text=chunk_row[\"text\"],\n",
    "        length=len(chunk_row[\"text\"]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    chunks = parse_with_llamaparse(INPUT_DIR)\n",
    "    if not chunks:\n",
    "        return\n",
    "    from collections import defaultdict\n",
    "    by_file: Dict[str, List[Dict[str, Any]]] = defaultdict(list)\n",
    "    for ch in chunks:\n",
    "        by_file[ch[\"file_name\"]].append(ch)\n",
    "    doc_map: Dict[str, uuid.UUID] = {fname: mk_uuid() for fname in by_file.keys()}\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    embs = embed_texts(client, texts)\n",
    "    for c, e in zip(chunks, embs):\n",
    "        c[\"embedding\"] = e\n",
    "    pg_conn = psycopg2.connect(**PG)\n",
    "    pg_conn.autocommit = False\n",
    "    neo_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "    try:\n",
    "        with pg_conn, pg_conn.cursor() as cur, neo_driver.session() as neo_sess:\n",
    "            for fname, group in by_file.items():\n",
    "                doc_id = doc_map[fname]\n",
    "                first = group[0]\n",
    "                doc_row = {\n",
    "                    \"id\": doc_id,\n",
    "                    \"file_name\": fname,\n",
    "                    \"title\": first.get(\"title\"),\n",
    "                    \"page_count\": first.get(\"page_count\"),\n",
    "                }\n",
    "                ensure_document(cur, doc_row)\n",
    "                neo4j_upsert_document(neo_sess, doc_row)\n",
    "                for ch in group:\n",
    "                    chunk_row = {\n",
    "                        \"id\": mk_uuid(),\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"page_number\": ch.get(\"page_number\"),\n",
    "                        \"chunk_index\": ch.get(\"chunk_index\", 0),\n",
    "                        \"text\": ch[\"text\"],\n",
    "                        \"embedding\": ch[\"embedding\"],\n",
    "                    }\n",
    "                    insert_chunk(cur, chunk_row)\n",
    "                    neo4j_upsert_page_and_chunk(neo_sess, doc_row, chunk_row)\n",
    "        pg_conn.commit()\n",
    "    except Exception:\n",
    "        pg_conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        neo_driver.close()\n",
    "        pg_conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
